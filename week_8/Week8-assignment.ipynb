{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d60e50e4cc9da06e03d8e077329cf24b",
     "grade": false,
     "grade_id": "cell-342b59512d5e4070",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Week 8 - Classification\n",
    "\n",
    "This week's assignment will give you an opportunity to practice your classification skills with the scikit-learn package. You will have to apply several different classification methods and learn about their parameters.\n",
    "\n",
    "In sklearn the different options (called hyperparameters) for each classifier are specofied in the constructor when you initialize an object. Then, method __.fit(X, y)__ will run the training procedure where the classifier will learn from a set of samples and their features in __X__ and the corresponding class labels in __y__. Once the classifier model is trained you can use it for predicting the labels on previously unseen data with __.predict(X_new)__.\n",
    "\n",
    "You will need to use various classification metrics to __evaluate the performance of the trained model__. See http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics for more details.\n",
    "\n",
    "A balanced F1 score combining precision and recall is a good default performance measure for binary classifiers. However, for instance as in the example with handwritten digits - the problem is multiclass - where we have 10 classes, one for each digit. In extending a binary metric to multiclass or multilabel problems, it could be treated as a collection of binary problems, one for each class. It is, therefore, necessary to average the evaluation results. There is then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where available, you should select among these using the average parameter. Read more about __\"micro\" and \"macro\" averaging of classification metrics__ in scikit-learn documentation.\n",
    "\n",
    "\"macro\" simply calculates the mean of the binary metrics, giving equal weight to each class. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class.\n",
    "\n",
    "Thus if we make sure that each digit is represented by approximately equal number of samples, the performance of a classifier will be best evaluated using F1 with macro averaging across the classes. __StratifiedKfold__ class or a stratified train_test_split() can be used to make sure each class gets an approximately equal number of samples in a random split.\n",
    "\n",
    "The final, and probably, one of most important aspects of machine learning covered by this assignment - is to __avoid overfitting the models while they train__. Overfitting leads to a classifier that is showing excellent performance on the dataset used for training, while its performance on a previously unseen dataset could be average if not poor. __Cross-validation__ is an important technique to master. Fortunately, even the complex scenarios of cross-validation are already implemented and available for use in sklearn.\n",
    "\n",
    "Often we need to choose between several classification models based on their performance, for instance the classifiers initialized with different hyperparameters, we need to make sure there is no overfitting of hyperparameters. It means that the choise of hyperparameters can be optimal for the training dataset but may not be optimal in general. __GridSearchCV__ class simplifies the hyperparameter optimization procedure, but you need to make sure to put aside a subset of your data for the final validation. __train_test_split()__ is the best way to split the dataset in sklearn.\n",
    "\n",
    "There is a lot to explore in scikit learn documentation, take this opportunity, and ask your questions on Slack channel #week8 if something is unclear.\n",
    "\n",
    "Due to a heavy use of randomization techniques the exact performance mertics may not be achieved (sometimes even with the random seed set) therefore the tests in assert statements will  validate your solutions roughly. Don't be surprised to get slightly different results when you restart your calculations.\n",
    "\n",
    "\n",
    "## Assignments\n",
    "\n",
    "There are 5 graded assignments and total 6 points: the first task is worth 2 points, the other four are 1 point each. Additionally, there is an ungraded assignment that we highly encourage you to solve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c3f53014b5876dbd1229e1779a49772c",
     "grade": false,
     "grade_id": "cell-8cac1e0680474a0d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1a43862c9797e2f5fdffa26f3efa0e49",
     "grade": false,
     "grade_id": "cell-b24ccf3f1f5aad98",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task #1  (2 points) - KNN\n",
    "\n",
    "Implement your own version of Nearest Neighbor classifier as a class MyKNN. The only hyperparameter to \\_\\_init\\_\\_(self, K) is K - the number of neighbors.\n",
    "\n",
    "Implement  .fit(X, y) method. There is no real training as the model simply memorizes all the data samples and their class labels.\n",
    "\n",
    "Implement .predict(X_new) method; this is where all the calculations are. You need to compare each sample in X_new to the memorized data and choose the K nearest neighbors using euclidean distance. Then you need to predict the label based on the most frequent label of the K neighbors and return an array of predicted labels. The problem could be binary or multiclass, it does not matter for the implementation, however you predict only one label for each sample, so it is not a multilabel classification problem.\n",
    "\n",
    "Do not worry about memory or speed optimization, we will not use MyKNN on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "3055364cb8db4ea8fb740decc477059c",
     "grade": false,
     "grade_id": "t1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "class MyKNN():\n",
    "    \"\"\"\n",
    "    Nearest neighbor classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, K):\n",
    "        \"\"\"\n",
    "        Initiate instance of the classifier with K nearest neighbors\n",
    "        \"\"\"\n",
    "        self.K = K\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model. Simply \"memorize\" the training data. \n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __distance(self, X_new_sample):\n",
    "        \"\"\"\n",
    "        Find distance between one sample and other points\n",
    "        \"\"\"\n",
    "        euclid_distances = []\n",
    "        # Calculate euclidean distance in 2D\n",
    "        for i in range(len(self.X)):\n",
    "            euclid_distances.append(distance.euclidean(self.X[i], X_new_sample))\n",
    "         \n",
    "        indices = np.argpartition(euclid_distances, self.K - 1, axis = 0) # The distance indices correspond to the indices of the\n",
    "        # nearest neighbors\n",
    "        # np.argpartition returns indices of smallest elements. These are the k nearest neighbors\n",
    "        predicted_labels = self.y[indices[:self.K]] # These are the predicted labels of the k nearest neighbors.\n",
    "        return(np.bincount(predicted_labels).argmax()) # Return the most frequent label\n",
    "        \n",
    "    def predict(self, X_new):\n",
    "        \"\"\"\n",
    "        Find nearest neighbors, using euclidean distance to choose neighbors. Implements __distance method\n",
    "        to avoid nested for loop\n",
    "        \"\"\"\n",
    "        predicted_labels = []\n",
    "        # could use .cdist() but alas it was not to be\n",
    "        for i in X_new:\n",
    "            predicted_labels.append(self.__distance(i))\n",
    "            # .__distance calculates distance between one sample in X_new and all other points\n",
    "        return(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "89bb981f883697425dd741d940cb8bff",
     "grade": true,
     "grade_id": "t1-test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "###### Synthetic example  ###############################\n",
    "# 6 samples, 2 features, binary class labels\n",
    "\n",
    "X = np.array([\n",
    "    [1.0, 2.5],\n",
    "    [3.5, 4.5],\n",
    "    [6.5, 6.5],\n",
    "    [4.5, 1.5],\n",
    "    [5.5, 3.0],\n",
    "    [7.5, 3.5]])\n",
    "y = np.array([0,0,0,1,1,1])\n",
    "\n",
    "#### K = 1 ###############\n",
    "\n",
    "cls = MyKNN(1)\n",
    "cls.fit(X, y)\n",
    "\n",
    "ref_cls = KNeighborsClassifier(1)\n",
    "ref_cls.fit(X, y)\n",
    "\n",
    "#### Prediction for the same dataset with K=1 should give accuracy 100%\n",
    "assert accuracy_score(y, ref_cls.predict(X)) == 1.\n",
    "assert accuracy_score(y, cls.predict(X)) == 1.\n",
    "\n",
    "\n",
    "#### K = 3 ###############\n",
    "cls = MyKNN(3)\n",
    "cls.fit(X, y)\n",
    "\n",
    "ref_cls = KNeighborsClassifier(3)\n",
    "ref_cls.fit(X, y)\n",
    "\n",
    "#### Prediction for the same dataset as used in training may not give you excellent results for K > 1\n",
    "assert accuracy_score(y, ref_cls.predict(X)) == 5./6.\n",
    "assert accuracy_score(y, cls.predict(X)) == 5./6.\n",
    "\n",
    "#### In this test we create a perturbation in input data\n",
    "# that should affect prediction accuracy\n",
    "assert accuracy_score(y, ref_cls.predict(X * 0.5)) == 3./6.\n",
    "assert accuracy_score(y, cls.predict(X * 0.5)) == 3./6.\n",
    "\n",
    "##### Iris dataset ##############################################\n",
    "# 150 samples, 4 features\n",
    "# 3 classes, 50 samples per class\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X_iris, y_iris = load_iris(return_X_y=True)\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(X_iris, y_iris, test_size=0.3, stratify=y_iris)\n",
    "cls_iris = MyKNN(5)\n",
    "cls_iris.fit(X_iris_train, y_iris_train)\n",
    "\n",
    "ref_cls_iris = KNeighborsClassifier(5)\n",
    "ref_cls_iris.fit(X_iris_train, y_iris_train)\n",
    "assert accuracy_score(y_iris_test, ref_cls_iris.predict(X_iris_test)) >= 0.9\n",
    "assert accuracy_score(y_iris_test, cls_iris.predict(X_iris_test)) >= 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "509d4db2753f9afe181f561f1adaa661",
     "grade": false,
     "grade_id": "cell-825e2acaf3ee4b45",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 2  (1 point) - Sampling\n",
    "\n",
    "Analyze stability of features in subsamples in train_test_split().\n",
    "Take the wines dataset (loaded below), and plot a distribution of \"quality\" feature. You will see that for wine quality __&lt;5 and &gt;7__ we only have a few samples; exclude these samples from wines dataset. Define __y__ as a vector of wine quality class labels. Define __X__ as the matrix of samples with the rest of the features (excluding quality).\n",
    "\n",
    "Run 1000 iterations of train_test_split() subsampling from __X__ and __y__ stratified by __y__ with 30% test size.\n",
    "\n",
    "Create __avg_alcohol_values_train__ list and append mean values of \"alcohol\" feature in the training subset on each iteration.\n",
    "\n",
    "Create __avg_alcohol_values_test__ list and append mean values of \"alcohol\" feature in the testing subset on each iteration.\n",
    "\n",
    "Analyze the mean and standard deviation of values in each of the lists. Plot histograms for visual aid.\n",
    "Think about why standard deviations of mean values from test and train samples differ and why the means are equal.\n",
    "\n",
    "The automatic tests will be checking X, y, and the avg_alcohol... lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aeba7faf9d2b3fcf7d76a1911e8c9bbf",
     "grade": false,
     "grade_id": "cell-ba7496856ec4a685",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE6xJREFUeJzt3V+MnNd93vHvY9KWZdERqcpdsKRQ8YJQIFmwHC1Upw6M\nZVhHdG2YuigEGkrABCqYC8ZwWhcF1ZsiF0R1URUNpKoAYaUmIMULlrVAwrHcMow2bYBKsmkrpUmJ\nEGNRERmKTKw/6aqCXKq/XuyrdMRKmtk/s6M98/0Ai3nfM+fM+R1y9pl3Xs68TFUhSWrXh0ZdgCRp\nuAx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuNWj7oAgGuvvbauv/76BY9//fXX\nueqqq5auoA+4cVsvuOZx4Zrn59ixY39VVZ/o27Gq3vcHuAF4uufnr4HfBq4BjgDPdbfresbcA5wG\nTgG395vj1ltvrcV4/PHHFzV+pRm39Va55nHhmucH+EH1ydeq6n/qpqpOVdUtVXULcCvwv4BHgT3A\n0araDBzt9klyI7ADuAnYBjyYZNVgr0+SpKU233P0W4E/q6oXgO3A/q59P3BHt70dmK6qN6vqeeaO\n7G9bimIlSfM336DfAXyr256oqvPd9kvARLe9AXixZ8zZrk2SNAKpAS9TnOQjwF8AN1XVhSSvVtXa\nnvtfqap1SR4Anqiqh7v2h4DHqurgZY+3C9gFMDExcev09PSCFzE7O8uaNWsWPH6lGbf1gmseF655\nfrZs2XKsqib79ZvPp26+APywqi50+xeSrK+q80nWAxe79nPAdT3jNnZt71BV+4B9AJOTkzU1NTWP\nUt5pZmaGxYxfacZtveCax4VrHo75nLr5Cv/vtA3AYWBnt70TONTTviPJFUk2AZuBpxZbqCRpYQY6\nok9yFfB54Dd7mu8FDiS5G3gBuBOgqk4kOQCcBC4Bu6vqrSWtWpI0sIGCvqpeB/7WZW0/Ze5TOO/W\nfy+wd9HVSZIWzUsgSFLjPhCXQJD6OX7uNX59zx+MZO4z935xJPNKS8UjeklqnEEvSY0z6CWpcQa9\nJDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zouaSR9QXshNS8UjeklqnEEv\nSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGjdQ0CdZm+RgkmeTPJPkF5Nck+RIkue623U9/e9JcjrJ\nqSS3D698SVI/gx7R/y7wvar6eeBTwDPAHuBoVW0Gjnb7JLkR2AHcBGwDHkyyaqkLlyQNpm/QJ7ka\n+BzwEEBV/ayqXgW2A/u7bvuBO7rt7cB0Vb1ZVc8Dp4HblrpwSdJgUlXv3yG5BdgHnGTuaP4Y8DXg\nXFWt7foEeKWq1iZ5AHiiqh7u7nsIeKyqDl72uLuAXQATExO3Tk9PL3gRs7OzrFmzZsHjV5pxWy/A\nxZdf48Ibo5n75g1Xj2TecVzzOD63F7PmLVu2HKuqyX79BrnWzWrgF4CvVtWTSX6X7jTN26qqkrz/\nK8Zlqmofcy8gTE5O1tTU1HyGv8PMzAyLGb/SjNt6Ae5/5BD3HR/NpZnO3DU1knnHcc3j+NxejjUP\nco7+LHC2qp7s9g8yF/wXkqwH6G4vdvefA67rGb+xa5MkjUDfoK+ql4AXk9zQNW1l7jTOYWBn17YT\nONRtHwZ2JLkiySZgM/DUklYtSRrYoO8Lvwo8kuQjwE+A32DuReJAkruBF4A7AarqRJIDzL0YXAJ2\nV9VbS165JGkgAwV9VT0NvNsJ/63v0X8vsHcRdUmSlojfjJWkxhn0ktQ4g16SGmfQS1LjDHpJapxB\nL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS\n1DiDXpIaZ9BLUuMMeklq3EBBn+RMkuNJnk7yg67tmiRHkjzX3a7r6X9PktNJTiW5fVjFS5L6m88R\n/ZaquqWqJrv9PcDRqtoMHO32SXIjsAO4CdgGPJhk1RLWLEmah8WcutkO7O+29wN39LRPV9WbVfU8\ncBq4bRHzSJIWYdCgL+APkxxLsqtrm6iq8932S8BEt70BeLFn7NmuTZI0AqsH7PdLVXUuyd8GjiR5\ntvfOqqokNZ+JuxeMXQATExPMzMzMZ/g7zM7OLmr8SjNu6wWYuBK+fvOlkcw9qj/rcVzzOD63l2PN\nAwV9VZ3rbi8meZS5UzEXkqyvqvNJ1gMXu+7ngOt6hm/s2i5/zH3APoDJycmamppa8CJmZmZYzPiV\nZtzWC3D/I4e47/igxyVL68xdUyOZdxzXPI7P7eVYc99TN0muSvLxt7eBXwF+DBwGdnbddgKHuu3D\nwI4kVyTZBGwGnlrqwiVJgxnkcGECeDTJ2/1/v6q+l+T7wIEkdwMvAHcCVNWJJAeAk8AlYHdVvTWU\n6iVJffUN+qr6CfCpd2n/KbD1PcbsBfYuujpJ0qL5zVhJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLU\nOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z\n6CWpcQa9JDXOoJekxhn0ktS4gYM+yaokP0rynW7/miRHkjzX3a7r6XtPktNJTiW5fRiFS5IGM58j\n+q8Bz/Ts7wGOVtVm4Gi3T5IbgR3ATcA24MEkq5amXEnSfA0U9Ek2Al8EvtHTvB3Y323vB+7oaZ+u\nqjer6nngNHDb0pQrSZqvVFX/TslB4F8BHwf+WVV9KcmrVbW2uz/AK1W1NskDwBNV9XB330PAY1V1\n8LLH3AXsApiYmLh1enp6wYuYnZ1lzZo1Cx6/0ozbegEuvvwaF94Yzdw3b7h6JPOO45rH8bm9mDVv\n2bLlWFVN9uu3ul+HJF8CLlbVsSRT79anqipJ/1eMd47ZB+wDmJycrKmpd33ogczMzLCY8SvNuK0X\n4P5HDnHf8b5P16E4c9fUSOYdxzWP43N7OdY8yLPos8CXk/xD4KPAzyV5GLiQZH1VnU+yHrjY9T8H\nXNczfmPXJkkagb7n6KvqnqraWFXXM/ePrH9UVb8KHAZ2dt12Aoe67cPAjiRXJNkEbAaeWvLKJUkD\nWcz7wnuBA0nuBl4A7gSoqhNJDgAngUvA7qp6a9GVSpIWZF5BX1UzwEy3/VNg63v02wvsXWRtkqQl\n4DdjJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4\ng16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhrXN+iTfDTJU0n+NMmJJL/T\ntV+T5EiS57rbdT1j7klyOsmpJLcPcwGSpPc3yBH9m8AvV9WngFuAbUk+A+wBjlbVZuBot0+SG4Ed\nwE3ANuDBJKuGUbwkqb++QV9zZrvdD3c/BWwH9nft+4E7uu3twHRVvVlVzwOngduWtGpJ0sAGOkef\nZFWSp4GLwJGqehKYqKrzXZeXgIluewPwYs/ws12bJGkEVg/SqareAm5JshZ4NMknL7u/ktR8Jk6y\nC9gFMDExwczMzHyGv8Ps7Oyixq8047ZegIkr4es3XxrJ3KP6sx7HNY/jc3s51jxQ0L+tql5N8jhz\n594vJFlfVeeTrGfuaB/gHHBdz7CNXdvlj7UP2AcwOTlZU1NTCyh/zszMDIsZv9KM23oB7n/kEPcd\nn9fTdcmcuWtqJPOO45rH8bm9HGse5FM3n+iO5ElyJfB54FngMLCz67YTONRtHwZ2JLkiySZgM/DU\nUhcuSRrMIIcL64H93SdnPgQcqKrvJPnvwIEkdwMvAHcCVNWJJAeAk8AlYHd36keSNAJ9g76q/gfw\n6Xdp/ymw9T3G7AX2Lro6SdKi+c1YSWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMM\neklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCX\npMb1Dfok1yV5PMnJJCeSfK1rvybJkSTPdbfresbck+R0klNJbh/mAiRJ72+QI/pLwNer6kbgM8Du\nJDcCe4CjVbUZONrt0923A7gJ2AY8mGTVMIqXJPXXN+ir6nxV/bDb/p/AM8AGYDuwv+u2H7ij294O\nTFfVm1X1PHAauG2pC5ckDWZe5+iTXA98GngSmKiq891dLwET3fYG4MWeYWe7NknSCKSqBuuYrAH+\nGNhbVd9O8mpVre25/5WqWpfkAeCJqnq4a38IeKyqDl72eLuAXQATExO3Tk9PL3gRs7OzrFmzZsHj\nV5pxWy/AxZdf48Ibo5n75g1Xj2Re17x8RrVeWNzv85YtW45V1WS/fqsHebAkHwb+E/BIVX27a76Q\nZH1VnU+yHrjYtZ8DrusZvrFre4eq2gfsA5icnKypqalBSnlXMzMzLGb8SjNu6wW4/5FD3Hd8oKfr\nkjtz19RI5nXNy2dU64Xl+X0e5FM3AR4Cnqmqf9Nz12FgZ7e9EzjU074jyRVJNgGbgaeWrmRJ0nwM\n8tL5WeDXgONJnu7a/gVwL3Agyd3AC8CdAFV1IskB4CRzn9jZXVVvLXnlkqSB9A36qvoTIO9x99b3\nGLMX2LuIuiRJS8RvxkpS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklq\nnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMb1\nDfokv5fkYpIf97Rdk+RIkue623U9992T5HSSU0luH1bhkqTBDHJE/01g22Vte4CjVbUZONrtk+RG\nYAdwUzfmwSSrlqxaSdK89Q36qvqvwMuXNW8H9nfb+4E7etqnq+rNqnoeOA3ctkS1SpIWIFXVv1Ny\nPfCdqvpkt/9qVa3ttgO8UlVrkzwAPFFVD3f3PQQ8VlUH3+UxdwG7ACYmJm6dnp5e8CJmZ2dZs2bN\ngsevNOO2XoCLL7/GhTdGM/fNG64eybyuefmMar2wuN/nLVu2HKuqyX79Vi/o0XtUVSXp/2rx/4/b\nB+wDmJycrKmpqQXXMDMzw2LGrzTjtl6A+x85xH3HF/10XZAzd02NZF7XvHxGtV5Ynt/nhX7q5kKS\n9QDd7cWu/RxwXU+/jV2bJGlEFhr0h4Gd3fZO4FBP+44kVyTZBGwGnlpciZKkxej7HinJt4Ap4Nok\nZ4F/CdwLHEhyN/ACcCdAVZ1IcgA4CVwCdlfVW0OqXZI0gL5BX1VfeY+7tr5H/73A3sUUJUlaOn4z\nVpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGjeaS+NpUY6fe41f3/MHI5n7\nzL1fHMm8khbOI3pJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqc34yVNPau\nH9E3zQG+ue2qoc/hEb0kNW5oQZ9kW5JTSU4n2TOseSRJ728op26SrAL+HfB54Czw/SSHq+rkMOYb\n1UW+vMCXpJVgWEf0twGnq+onVfUzYBrYPqS5JEnvY1hBvwF4sWf/bNcmSVpmqaqlf9DkHwHbquof\nd/u/Bvy9qvqtnj67gF3d7g3AqUVMeS3wV4sYv9KM23rBNY8L1zw/f7eqPtGv07A+XnkOuK5nf2PX\n9jeqah+wbykmS/KDqppcisdaCcZtveCax4VrHo5hnbr5PrA5yaYkHwF2AIeHNJck6X0M5Yi+qi4l\n+S3gPwOrgN+rqhPDmEuS9P6G9s3Yqvou8N1hPf5lluQU0AoybusF1zwuXPMQDOUfYyVJHxxeAkGS\nGrdigz7JR5M8leRPk5xI8jujrmm5JFmV5EdJvjPqWpZDkjNJjid5OskPRl3PckiyNsnBJM8meSbJ\nL466pmFKckP39/v2z18n+e1R1zVMSf5Jl10/TvKtJB8d2lwr9dRNkgBXVdVskg8DfwJ8raqeGHFp\nQ5fknwKTwM9V1ZdGXc+wJTkDTFbV2Hy+Osl+4L9V1Te6T659rKpeHXVdy6G7hMo55r5788Ko6xmG\nJBuYy6wbq+qNJAeA71bVN4cx34o9oq85s93uh7uflfmqNQ9JNgJfBL4x6lo0HEmuBj4HPARQVT8b\nl5DvbAX+rNWQ77EauDLJauBjwF8Ma6IVG/TwN6cwngYuAkeq6slR17QM/i3wz4H/M+pCllEBf5jk\nWPeN6tZtAv4S+A/dKbpvJBn+Rcs/OHYA3xp1EcNUVeeAfw38OXAeeK2q/suw5lvRQV9Vb1XVLcx9\n8/a2JJ8cdU3DlORLwMWqOjbqWpbZL3V/z18Adif53KgLGrLVwC8A/76qPg28DozFpb6701RfBv7j\nqGsZpiTrmLvQ4ybg7wBXJfnVYc23ooP+bd3b2seBbaOuZcg+C3y5O2c9DfxykodHW9LwdUc/VNVF\n4FHmro7asrPA2Z53qAeZC/5x8AXgh1V1YdSFDNk/AJ6vqr+sqv8NfBv4+8OabMUGfZJPJFnbbV/J\n3LXvnx1tVcNVVfdU1caqup65t7d/VFVDOwr4IEhyVZKPv70N/Arw49FWNVxV9RLwYpIbuqatwFD+\nL4cPoK/Q+Gmbzp8Dn0nyse6DJVuBZ4Y12Ur+P2PXA/u7f6H/EHCgqsbi44ZjZgJ4dO53gdXA71fV\n90Zb0rL4KvBIdyrjJ8BvjLieoeteyD8P/Oaoaxm2qnoyyUHgh8Al4EcM8RuyK/bjlZKkwazYUzeS\npMEY9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNe7/Aj9sufXlZpsEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff81fc22438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wines = pd.read_csv(url, sep=\";\")\n",
    "wines[\"quality\"].hist()\n",
    "wines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "c00dccbb65cf0cfffcd86ded25c8323f",
     "grade": false,
     "grade_id": "t2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# wines.loc[wines[\"quality\"] == 7] # see if there are values where quality equals 7. \n",
    "wines = wines.query(\"quality != [3, 4, 8]\")\n",
    "# or\n",
    "# wines = wines[(wines.quality <= 7) & (wines.quality >= 5)]\n",
    "#X = wines.loc[:, wines.columns != \"quality\"]\n",
    "# Useful ref: https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/\n",
    "y = wines[\"quality\"]\n",
    "X = wines.drop(\"quality\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training standard deviation: 0.0150646766956\n",
      "Testing standard deviation: 0.0350848391464\n",
      "Training mean: 10.4110699153\n",
      "Testing mean: 10.4133634868\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGbhJREFUeJzt3X+UX3V95/HnywQhBFQw7GxIIoNtdBs6ki5zoi5qBxEJ\nYA1Yl4ZSDatupIsebNPTDbpr6bbxcOxGe6wijSsHjvyI2aMsWWJrY8po2RUQMBoCpAQYDklDUn4z\nSFkS3vvH/Qxcvn4n8/1xv9/vzHxej3O+Z+733vu5n/e9933fc+/93vmOIgIzM5v+XtPrAMzMrDtc\n8M3MMuGCb2aWCRd8M7NMuOCbmWXCBd/MLBMu+BWSNCrpzT3oNyT9apvLGJH0vl71X2eZ/Wm5M6tc\nrrXGuV2dXuZ2Twu+pGFJT0o6tEv9DUnaVdGyhiV9ojwuIo6IiAerWL5Nbc5tm4x6VvAl9QPvBgL4\nYK/iMKuac9smrYjoyQv4PPB/gC8BN5XGvx14FJhRGncO8PM0PAu4GngSuBf4Y2BXA/3NBp4HXgJG\n0+tYil96q4EHgMeBDcDRqc1hwDVp/FPAT4A+YA1wAPiXtJyvpvkD+NU0fBXwNWAT8CxwG/ArpXje\nD+wAngYuB34IfGKc2JcAP04x7AG+Cry2NL3c7yxgLfBwWvYtwKw07YPA9rScYeDXSssYAf4I+Hlq\n923gsNL0/wjsBJ4ANgLH1uu/Ju7fAe6oGfcHwMY0fBbwU+AZ4BHg0tJ8/Wm5M0vxva80/VLgmtL7\ndwD/N63bz4Ch0rQLgAfTfngION+57dzOMbd7WfB3Av8JOAl4EegrTXsAOK30/n8Cq9PwZSmBjgLm\np5044UGR2g7VzgtcDNyalnUo8NfA9WnaJ4H/DRwOzEixvi5NG65NYn75oHg8JfRM4FpgfZo2JyXC\nh9K0i9M2GO+gOCnt9JkpWe4FPjNOv19Lsc1LMf+7tF5vAZ4DTgMOoSgmO0kHV0q62ykKxdGpjwvT\ntPcCjwH/Ni3rr4AfNXBQHJ4ScWFp3E+A5aX9MUBRmN4G7AXObvagSOv6OHBmWtZp6f0xFMXwGeCt\nad65wAnObed2jrndq2L/rpQEc9L7+4A/KE3/c+DKNHxk2pnHpfcPAqeX5v0E7R0U9wKnlt7PTbHN\nBD5G8Zv1bXWWNVybxPzyQfE/StPOBO5Lwx8FflyaJoqzgLoHRZ2+PwPcUNtvSojngRPrtPmvwIbS\n+9cAu0lnCynpfq80/YvAFWn4m8AXS9OOSNuo/2AHRZp2DfD5NLyQ4iA5fJx5/xL4cgsHxX8GvlWz\nrO8DK9JB8RTw26SzQee2czvX3O7VPfwVwN9FxGPp/XVpHKX3H0ofeH0IuCsiHk7TjqVIoDHl4VYc\nB9wg6SlJT1EcJAcoLm+/RbFx10v6J0lflHRIE8t+tDT8C4pkgpp1iGIvjvuBm6S3SLpJ0qOSngG+\nQHEmVWsOxaX6A3WmHUtxKTzW50sphnkNxltuO0pxllFuO57rgPPS8O8C/ysifpHW6+2Sbpb0z5Ke\nBi4cZ70mchzw78f2YdqP7wLmRsRzFJffFwJ7JG2S9G9a6KNRzm3n9qTN7a4XfEmzgHOB30w7+VGK\ne18nSjoRICLuodgJZ1BsyOtKi9hDcYk6ZkET3UedcY8AZ0TEG0qvwyJid0S8GBF/GhGLKC4fP0Bx\nBjPeshr1qnWQJF69TrW+TnGmuDAiXgd8luLMqdZjFPdef6XOtH+iSJ5ynwsozoQmUtt2NvDGBttu\nBo6RtJji4Cjvy+so7pkuiIjXA1dQf72gOBM+vPT+X5eGH6E4Cyrvw9kRcRlARHw/Ik6jOMO9D/hG\nA3E3zbkNOLfHTMrc7sUZ/tkUZxmLgMXp9WvAP/BKwkGxwS4G3kNxn3PMBuASSUdJmgd8qom+9wJv\nlPT60rgrgDWSjgOQdIykZWn4FEkDkmZQ3Ct7keKDsbFltfpc8iZgQNLZ6Vnci3j1Tq51ZOp/NP0G\n//16M6UzmyuBL0k6VtIMSe9MZ5MbgLMknZrO5FYBL1Bc1k/keuA/SFqclvUF4LaIGJmoYUS8SLH/\n/oLi/unmmvV6IiL+RdISigI4nq3AckmHSBoEPlyadg3wW5JOT+t8WHpMcb6kPknL0oH8AsUHkS/V\nWX4VnNvO7fJ6Tb7cbuS+T5Uv4G+BtXXGn0tx2TV2X+tNKfhNNfPNprgcHbtE/S/AA6XpfwN89iD9\nX8krTyaMPcnwhxRPFTxLccn4hTTveWn8cxQHwVdK8b0T+EeKJyq+EvXvc/55qd8hSvdYgaWp/diT\nDD8GPjJOzO+h+O09SlE8/htwS2l6ud9ZFPcLd6dl/4hXnmQ4B7gnjf8hpQ94mPhJgQvTtnkCuAmY\nX6//ceIfe0TxazXjP0xxtvtsWuZXeeXeZT+vvs/5ZoqnQUYpispXauJ7e1qnJ4B/TvO8ieLM54dp\nncee4Fjk3HZu55jbSgucsiT9PsUn47/Z61haJek1FPc5z4+Im3sdj00Ozm2r2pT7agVJcyWdLOk1\nkt5Kcfl2Q6/jala6RHtDuowcu295a4/Dsh5yblunTcXvKXktxfPEx1NcxqynuGycat5JcS/3tRSX\nomdHxPO9Dcl6zLltHTXlb+mYmVljptwtHTMza82kuKUzZ86c6O/v70nfzz33HLNnz3bf07zvO++8\n87GIOKarndK73O7l/q3H8UyslZiazutOPJ7W7Oukk06KXrn55pvddwZ9U/NFV9169Sq3e7l/63E8\nE2slpmbz2rd0zMwy4YJvZpYJF3wzs0y44JuZZcIF38wsEy74ZmaZcME3M8uEC76ZWSZc8M3MMjEp\nvlohZ/2rNzXdZuSyszoQiVnvtXI8gI+JRvkM38wsEy74ZmaZcME3M8uEC76ZWSZc8M3MMuGCb2aW\nCRd8M7NMuOCbmWXCBd/MLBMu+GZmmXDBNzPLhAu+mVkmXPDNzDLhgm9mlokJC76kBZJulnSPpO2S\nLk7jL5W0W9LW9Dqz1OYSSTsl7ZB0eidXwKxVzm3LTSPfh78fWBURd0k6ErhT0uY07csR8d/LM0ta\nBCwHTgCOBX4g6S0RcaDKwM0q4Ny2rEx4hh8ReyLirjT8LHAvMO8gTZYB6yPihYh4CNgJLKkiWLMq\nObctN039xytJ/cBvALcBJwOflvRR4A6KM6UnKQ6YW0vNdlHnIJK0ElgJ0NfXx/DwcPPRV2B0dLSn\nfa8aaP7ksIp4e73evep7PNMttyfbNm40nlUD+1tafrPrOtm2D3QnpoYLvqQjgO8An4mIZyR9Hfgz\nINLPtcDHGl1eRKwD1gEMDg7G0NBQE2FXZ3h4mF72vfaW55puN3L+UCV957jN65mOuT3ZtnGj8VzQ\n6r84bPKYmGzbB7oTU0NP6Ug6hOKAuDYivgsQEXsj4kBEvAR8g1cubXcDC0rN56dxZpOOc9ty0shT\nOgK+CdwbEV8qjZ9bmu0c4O40vBFYLulQSccDC4HbqwvZrBrObctNI7d0TgY+AmyTtDWN+yxwnqTF\nFJe9I8AnASJiu6QNwD0UT0Fc5KcYbJJybltWJiz4EXELoDqTvneQNmuANW3EZdZxzm3Ljf/S1sws\nEy74ZmaZcME3M8uEC76ZWSZc8M3MMuGCb2aWCRd8M7NMuOCbmWXCBd/MLBMu+GZmmXDBNzPLhAu+\nmVkmXPDNzDLhgm9mlgkXfDOzTLjgm5llwgXfzCwTLvhmZplwwTczy4QLvplZJlzwzcwy4YJvZpYJ\nF3wzs0y44JuZZcIF38wsEy74ZmaZcME3M8uEC76ZWSYmLPiSFki6WdI9krZLujiNP1rSZkn3p59H\nldpcImmnpB2STu/kCpi1yrltuWnkDH8/sCoiFgHvAC6StAhYDWyJiIXAlvSeNG05cAKwFLhc0oxO\nBG/WJue2ZWXCgh8ReyLirjT8LHAvMA9YBlydZrsaODsNLwPWR8QLEfEQsBNYUnXgZu1ybltumrqH\nL6kf+A3gNqAvIvakSY8CfWl4HvBIqdmuNM5s0nJuWw5mNjqjpCOA7wCfiYhnJL08LSJCUjTTsaSV\nwEqAvr4+hoeHm2lemdHR0Ur63rb76abb9M2CVQPN91VFvFWt91Tru57pmNuTbRs3Gs+qgf0tLf+v\nrr2xqfn7ZhVtBua9vqX+OqEb+6yhgi/pEIoD4tqI+G4avVfS3IjYI2kusC+N3w0sKDWfn8a9SkSs\nA9YBDA4OxtDQUGtr0Kbh4WGq6PuC1ZuabrNqYD9rtzX8O/dlI+cPNd2mVlXrPdX6rjVdc3sybWNo\nPJ5WjqNWjB17VRxLVenGPmvkKR0B3wTujYgvlSZtBFak4RXAjaXxyyUdKul4YCFwe3Uhm1XDuW25\naeT08mTgI8A2SVvTuM8ClwEbJH0ceBg4FyAitkvaANxD8RTERRFxoPLIM9bf4lnQyGVnVRzJlOfc\n7pDaHF01sL9rZ+82vgkLfkTcAmicyaeO02YNsKaNuMw6zrltufFf2pqZZcIF38wsEy74ZmaZcME3\nM8uEC76ZWSZc8M3MMuGCb2aWCRd8M7NMuOCbmWXCBd/MLBMu+GZmmXDBNzPLhAu+mVkmXPDNzDLh\ngm9mlgkXfDOzTLjgm5llwgXfzCwTLvhmZplwwTczy4QLvplZJlzwzcwy4YJvZpYJF3wzs0y44JuZ\nZcIF38wsEy74ZmaZcME3M8uEC76ZWSYmLPiSrpS0T9LdpXGXStotaWt6nVmadomknZJ2SDq9U4Gb\ntcu5bblp5Az/KmBpnfFfjojF6fU9AEmLgOXACanN5ZJmVBWsWcWuwrltGZmw4EfEj4AnGlzeMmB9\nRLwQEQ8BO4ElbcRn1jHObcvNzDbaflrSR4E7gFUR8SQwD7i1NM+uNO6XSFoJrATo6+tjeHi4jVBa\nNzo6Wknfqwb2N92mb1Zr7VpVXs+q1rsVvey7QVM+t3u9jWvzutu5PpGxeCZTHnZjn7Va8L8O/BkQ\n6eda4GPNLCAi1gHrAAYHB2NoaKjFUNozPDxMFX1fsHpT021WDexn7bZ2fuc2Z+T8oZeHq1rvVvSy\n7wZMi9zu9TauPR66nesTGYunfEz0Wjf2WUtP6UTE3og4EBEvAd/glUvb3cCC0qzz0zizKcG5bdNZ\nSwVf0tzS23OAsaccNgLLJR0q6XhgIXB7eyGadY9z26azCa+xJF0PDAFzJO0C/gQYkrSY4rJ3BPgk\nQERsl7QBuAfYD1wUEQc6E7pZe5zblpsJC35EnFdn9DcPMv8aYE07QZl1g3PbcuO/tDUzy4QLvplZ\nJlzwzcwy4YJvZpYJF3wzs0y44JuZZcIF38wsEy74ZmaZcME3M8uEC76ZWSZc8M3MMuGCb2aWCRd8\nM7NMuOCbmWXCBd/MLBMu+GZmmXDBNzPLhAu+mVkmXPDNzDIx4f+0temjf/Wml4dXDezngtL7gxm5\n7KxOhWSTXH+DOWJTg8/wzcwy4YJvZpYJF3wzs0y44JuZZcIF38wsEy74ZmaZcME3M8uEn8Ov4eeO\nzfLR6vE+Vf82xWf4ZmaZmLDgS7pS0j5Jd5fGHS1ps6T708+jStMukbRT0g5Jp3cqcLN2ObctN42c\n4V8FLK0ZtxrYEhELgS3pPZIWAcuBE1KbyyXNqCxas2pdhXPbMjJhwY+IHwFP1IxeBlydhq8Gzi6N\nXx8RL0TEQ8BOYElFsZpVyrltuWn1Q9u+iNiThh8F+tLwPODW0ny70rhfImklsBKgr6+P4eHhFkNp\nz+jo6Kv6XjWwv2t9983qbn+t9l31vqnd5pPMtMjtqrZxVfnZy1yvp914OrFPu3FctP2UTkSEpGih\n3TpgHcDg4GAMDQ21G0pLhoeHKffd6DdIVmHVwH7WbuvNg1LN9D1y/lClfddu88lqKud2Vdu4quOh\nl7leT7vxVH1MQHeOi1af0tkraS5A+rkvjd8NLCjNNz+NM5sqnNs2bbVa8DcCK9LwCuDG0vjlkg6V\ndDywELi9vRDNusq5bdPWhNc0kq4HhoA5knYBfwJcBmyQ9HHgYeBcgIjYLmkDcA+wH7goIg50KHaz\ntji3LTcTFvyIOG+cSaeOM/8aYE07QZl1g3PbcuO/tDUzy4QLvplZJlzwzcwy4YJvZpYJF3wzs0y4\n4JuZZcIF38wsEy74ZmaZcME3M8uEC76ZWSZc8M3MMuGCb2aWCRd8M7NMuOCbmWXCBd/MLBMu+GZm\nmXDBNzPLhAu+mVkmXPDNzDLhgm9mlgkXfDOzTLjgm5llwgXfzCwTLvhmZplwwTczy4QLvplZJlzw\nzcwy4YJvZpYJF3wzs0zMbKexpBHgWeAAsD8iBiUdDXwb6AdGgHMj4sn2wjTrLue2TUdVnOGfEhGL\nI2IwvV8NbImIhcCW9N5sKnJu27TSiVs6y4Cr0/DVwNkd6MOsF5zbNqUpIlpvLD0EPE1x2fvXEbFO\n0lMR8YY0XcCTY+9r2q4EVgL09fWdtH79+pbjaMfo6ChHHHHEy++37X66a333zYK9z3etu5b7Hpj3\n+kr7rt3m3XDKKafcWTpTn9BUz+2qtnFVx0Mvc72eduOp+piA1vZZs3nd1j184F0RsVvSvwI2S7qv\nPDEiQlLd3ygRsQ5YBzA4OBhDQ0NthtKa4eFhyn1fsHpT1/peNbCftdva3QWd73vk/KFK+67d5pPU\nlM7tqrZxVcdDL3O9nnbjqfqYgO4cF23d0omI3ennPuAGYAmwV9JcgPRzX7tBmnWbc9umo5YLvqTZ\nko4cGwbeD9wNbARWpNlWADe2G6RZNzm3bbpq5xqrD7ihuJXJTOC6iPhbST8BNkj6OPAwcG77YZp1\nlXPbpqWWC35EPAicWGf848Cp7QRl1kvObZuu/Je2ZmaZcME3M8vE5HlOqmL9DT5Otmpgf1cfxTSz\nqa/R+lI2ctlZHYikOdO24Ft1WklumBwJbq9odT/a9OFbOmZmmXDBNzPLhAu+mVkmXPDNzDLhgm9m\nlgkXfDOzTLjgm5llwgXfzCwTLvhmZplwwTczy4QLvplZJlzwzcwy4YJvZpYJF3wzs0y44JuZZcIF\n38wsEy74ZmaZ8H+8so4Z7z8sTfRvJf2fsg6ukf9c5X/dafX4DN/MLBOT/gzf/4fTzKaDiWrZeFdl\nVV7x+gzfzCwTLvhmZplwwTczy4QLvplZJib9h7aWn1Y/qJ9qj3P6gQTrto6d4UtaKmmHpJ2SVneq\nH7Nucl7bVNaRgi9pBvA14AxgEXCepEWd6MusW5zXNtV16gx/CbAzIh6MiP8HrAeWdagvs25xXtuU\npoiofqHSh4GlEfGJ9P4jwNsj4lOleVYCK9PbtwI7Kg+kMXOAx9z3tO/7uIg4pp0FNJLXafxkyO1e\n7t96HM/EWompqbzu2Ye2EbEOWNer/sdIuiMiBt13Hn13w2TI7cm2jR3PxLoRU6du6ewGFpTez0/j\nzKYy57VNaZ0q+D8BFko6XtJrgeXAxg71ZdYtzmub0jpySyci9kv6FPB9YAZwZURs70RfFejlpbf7\nnkKc121xPBPreEwd+dDWzMwmH3+1gplZJlzwzcwyMa0KvqQrJe2TdHdp3NGSNku6P/08qk67wyTd\nLulnkrZL+tPStEsl7Za0Nb3OrLLv0rwzJP1U0k3Ntu9Q3x1fb0kjkral5d/RSuzTVYdy+dul/Tki\naWsa3y/p+dK0K6qKpzRvU/kt6RIVX1+xQ9LpXYrnLyTdJ+nnkm6Q9IYeb59xj8GJts94plXBB64C\nltaMWw1siYiFwJb0vtYLwHsj4kRgMbBU0jtK078cEYvT63sV9z3mYuDeFtt3om/oznqfkpZffv64\nmfbT1VVUnMsR8Ttj+xP4DvDdUrsHSvv6wgrjGdNwfqv4uorlwAmpz8tVfK1Fp+PZDPx6RLwN+Efg\nktK0XmwfqHMMNrh96ouIafUC+oG7S+93AHPT8FxgxwTtDwfuovgLSoBLgT/qZN8Uz3NvAd4L3NRK\n7B3ouxvrPQLMqTO+qX02XV9V53JpvIBHgIX1+ulijtVtT1FoLynN933gnZ2Op2aec4Bre7x96h6D\njW6feq/pdoZfT19E7EnDjwJ99WZKl1RbgX3A5oi4rTT50+ky78omby801Dfwl8AfAy+12L4TfUPn\n1zuAH0i6U8XXETTbPjdV5DLAu4G9EXF/adzx6bbBDyW9u8p4aD6/51H8QhqzK43rdDxlHwP+pvS+\nF9sH6h+DrW6fLAr+y6L4dVj3OdSIOBDFpe58YImkX0+Tvg68meLyeA+wtsq+JX0A2BcRd7Yae4f6\n7uh6J+9K2/wM4CJJ72myfbZazOUx5wHXl97vAd6U2vwhcJ2k11URTzfyu+p4JH0O2A9cm0b1avtU\ncgyW5VDw90qaC5B+7jvYzBHxFHAz6X5cROxNB9BLwDcovjGxyr5PBj4oaYTi2xffK+maVmKvsu8u\nrDcRsTv93AfcUOqjnfWeztrK5dRuJvAh4Nul+V6IiMfT8J3AA8BbKoqnlfxu9Sss2o0HSRcAHwDO\nT0W6Z9vnIMdgy1/xkUPB3wisSMMrgBtrZ5B0TOkT+VnAacB96f3c0qznAHfXtm+n74i4JCLmR0Q/\nxQcxfx8Rv9do+0713en1ljRb0pFjw8D7S320s97TWVu5nLwPuC8idtW0mZGG3wwsBB6sIp4W83sj\nsFzSoZKOT/Hc3ul4JC2luLXywYj4xVibXm2fgxyDrW6f6fWhLcVl6h7gRYr7Wh8H3kjxgcj9wA+A\no9O8xwLfS8NvA34K/Dxt1M+XlvktYFuatpH0IUxVfdcsY4hXf2hTt32X+u7oelNcqv4svbYDn2t2\nvafzqxO5nKZfBVxYM+630z7YSvEh72/1Or+Bz1GcSe8AzuhSPDsp7o1vTa8rerx9xj0GJ9o+4738\n1QpmZpnI4ZaOmZnhgm9mlg0XfDOzTLjgm5llwgXfzCwTLvhmZplwwTczy8T/B4iUybF2yyRAAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff81f6b8198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def model(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, stratify = y)\n",
    "    avg_alcohol_values_train.append(X_train[\"alcohol\"].mean())\n",
    "    avg_alcohol_values_test.append(X_test[\"alcohol\"].mean())\n",
    "    \n",
    "avg_alcohol_values_train = []\n",
    "avg_alcohol_values_test = []\n",
    "\n",
    "for i in range(1000):\n",
    "    model(X,y)\n",
    "\n",
    "# My testing:\n",
    "print(\"Training standard deviation:\", np.std(avg_alcohol_values_train))\n",
    "print(\"Testing standard deviation:\", np.std(avg_alcohol_values_test))\n",
    "print(\"Training mean:\", np.mean(avg_alcohol_values_train))\n",
    "print(\"Testing mean:\", np.mean(avg_alcohol_values_test))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "pd.Series(avg_alcohol_values_test).hist(ax=ax1)\n",
    "pd.Series(avg_alcohol_values_train).hist(ax=ax2)\n",
    "ax1.set_title(\"Avg. testing alcohol values\")\n",
    "ax2.set_title(\"Avg. testing alcohol values\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4b883448456eb3581daaea89ac595eea",
     "grade": true,
     "grade_id": "t2-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert y.shape == (1518,)\n",
    "assert X.shape == (1518, 11)\n",
    "\n",
    "assert 0.034 <= round(pd.Series(avg_alcohol_values_test).std(), 3) <= 0.037\n",
    "assert round(pd.Series(avg_alcohol_values_test).mean(), 1) == round(pd.Series(avg_alcohol_values_train).mean(), 1)\n",
    "\n",
    "assert 0.014 <= round(pd.Series(avg_alcohol_values_train).std(), 3) <= 0.016\n",
    "assert round(pd.Series(avg_alcohol_values_train).mean(), 1) == round(wines['alcohol'].mean(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5560d561a5f36a659ef364ebb63f6726",
     "grade": false,
     "grade_id": "cell-6e46df97f5e597d8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 3 (1 point) - Multinomial NB\n",
    "\n",
    "Use RepeatedStratifiedKfold cross-validation to evaluate performance of a Multinomial Naive Bayes on the wine dataset predicting wine quality. As in the previous task exclude wines with quality that is not 5,6 or 7; and prepare sample-feature matrix X and vector with class labels y accordingly.\n",
    "\n",
    "Use the following classesfrom sklearn:  RepeatedStratifiedKFold, MultinomialNB and cross_val_score function.\n",
    "\n",
    "Initialize Repeated 5-fold cross validation object (5 splits) with 1000 repetitions.\n",
    "Initialize Multinomial NB classifier.\n",
    "Calculate cross validation scores for the NB classifier and the kfold cross validation object that we created (use cv parameter to pass a custom cross validation object to cros_val_score function). For scoring use F1 score with macro averageing 'f1_macro'. If you want to speed things up set n_jobs=-1  which would use the power of multiple CPU cores on your computer for the calculation.\n",
    "\n",
    "Store the results of cross validation scores in \"scores\" variable. We will check it in assert tests, as well as X and y will be tested.\n",
    "\n",
    "For visual aid plot a distribution of scores across cross validation runs. Now you have a robust estimate of Multinomial NB classifier performance in predicting wine quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7ad3fb1c62caac05fc258f474d9b8d04",
     "grade": false,
     "grade_id": "cell-f150c53545a8aa82",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wines = pd.read_csv(url, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bfe37efa32c2bab16b0bb20c4c6791c9",
     "grade": false,
     "grade_id": "cell-762846b2ba96f881",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wines = pd.read_csv(url, sep=\";\")\n",
    "wines = wines.query(\"quality != [3, 4, 8]\")\n",
    "y = wines[\"quality\"]\n",
    "X = wines.drop(\"quality\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing scoring in loop: 0.449547201683\n",
      "Testing scoring with rskf object: 0.449547201683\n",
      "Speed (s) with default n_job setting: 1.1804117549982038\n",
      "And using n_jobs = -1: 0.8175660980014072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "import timeit\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 100, random_state = 121222)\n",
    "# http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html\n",
    "# Repeats Stratified K-Fold n times with different randomization in each repetition.\n",
    "# Simple explanation on cross validation\n",
    "# https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "scores = cross_val_score(mnb, X, y, cv = rskf, scoring = \"f1_macro\", n_jobs = -1) \n",
    "# F score is a combination of precision and recall.\n",
    "# scores will contain a score for each split as generated by rskf. \n",
    "\n",
    "scores_test = []\n",
    "# What is rskf doing?\n",
    "# something like this:\n",
    "for train_index, test_index in rskf.split(X, y): # The rskf object is an iterable with all splits.\n",
    "    mnb = MultinomialNB()\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    mnb.fit(X_train, y_train)\n",
    "    y_predictions = mnb.predict(X_test)\n",
    "    scores_test.append(f1_score(y_test, y_predictions, average = \"macro\"))\n",
    "    \n",
    "print(\"Testing scoring in loop:\", np.mean(scores_test))\n",
    "print(\"Testing scoring with rskf object:\", np.mean(scores))\n",
    "# Yay its the same\n",
    "\n",
    "# what time difference does n_jobs make?\n",
    "\n",
    "def time_test():\n",
    "    score = cross_val_score(mnb, X, y, cv = rskf, scoring = \"f1_macro\")\n",
    "    return score\n",
    "\n",
    "def time_test_2():\n",
    "    score = cross_val_score(mnb, X, y, cv = rskf, scoring = \"f1_macro\", n_jobs = -1)\n",
    "    return score\n",
    "print(\"Speed (s) with default n_job setting:\", timeit.timeit(stmt = time_test, number = 1)) \n",
    "print(\"And using n_jobs = -1:\", timeit.timeit(stmt = time_test_2, number = 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "589995734ac5e8c080c37119bccf94b6",
     "grade": true,
     "grade_id": "cell-666c6eb3182f9908",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert y.shape == (1518,)\n",
    "assert X.shape == (1518, 11)\n",
    "assert  .4 < pd.Series(scores).mean() < .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e5b3aa4eefb141b86bb1a3795d6eeabf",
     "grade": false,
     "grade_id": "cell-3534396446b0c46b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 4 (1 point) - Logistic Regression\n",
    "\n",
    "Use RepeatedStratifiedKfold cross-validation to evaluate performance of a Logistic Regression on the wine dataset predicting wine quality. \n",
    "\n",
    "This time let us turn it into a binary classification problem and prepare a vector with class labels y that will only have 1 (for high-quality wine where quality >= 7) and 0 for the rest of wine samples.\n",
    "Initialize sample-feature matrix X (don't forget to exclude quality column).\n",
    "\n",
    "Use the following classesfrom sklearn:  RepeatedStratifiedKFold, LogisticRegression and cross_val_score function.\n",
    "\n",
    "As in the previous example: \n",
    "\n",
    "Initialize Repeated 5-fold cross validation object (5 splits) with 1000 repetitions.\n",
    "Initialize Logistic Regression classifier.\n",
    "Calculate cross validation scores for the Logistic Regression classifier and the kfold cross validation object that we created (use cv parameter to pass a custom cross validation object to cros_val_score function).\n",
    "\n",
    "However, for scoring let us use accuracy this time, and since it is a binary classification problem no averageing across classes is required. If you want to speed things up set n_jobs=-1  which would use the power of multiple CPU cores on your computer for the calculation.\n",
    "\n",
    "Store the results of cross validation scores in \"scores\" variable. We will check it in assert tests, as well as X and y will be tested.\n",
    "\n",
    "For visual aid plot a distribution of scores across cross validation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "829c31f4a67944462d7d9a1baea8b995",
     "grade": false,
     "grade_id": "cell-69bbd166ac4a50a5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wines = pd.read_csv(url, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "368dcd76be58e9af7c2b673198df4be5",
     "grade": false,
     "grade_id": "cell-d707c326d2bdb279",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wines = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "# Old way:\n",
    "# wines.loc[wines[\"quality\"] < 7, \"quality\"] = 0\n",
    "# wines.loc[wines[\"quality\"] >= 7, \"quality\"] = 1\n",
    "# But a better way is\n",
    "\n",
    "y = pd.Series(np.where(wines[\"quality\"] >= 7, 1, 0))\n",
    "X = wines.drop(\"quality\", axis = 1)\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 1000, random_state = 1)\n",
    "# For different types of scoring...\n",
    "# http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(lr, X, y, cv = rskf, n_jobs = -1) \n",
    "# setting cv = rskf allows cross validation to \n",
    "# take care of using the rskf class. Basically fivides cross_val_score for the data.\n",
    "# The first argument is the model to use. Second is data to fit model to. Third is class labels.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "db70f851842a179e269727c9d897189e",
     "grade": true,
     "grade_id": "cell-42b2feb509ff1a9d",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert y.shape == (1599,)\n",
    "assert X.shape == (1599, 11)\n",
    "assert  pd.Series(scores).mean() > 0.86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0dc0774f1a40ade484032213fd8dd434",
     "grade": false,
     "grade_id": "cell-e6886c0f1a409bf0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 5 (1 point) - Random Forest\n",
    "\n",
    "Let us use the digits dataset and apply Random Forest to predict the digit given a low resolution handwritten image.\n",
    "\n",
    "Use the following classes from sklearn:  RandomForestClassifier\n",
    "\n",
    "You will need to split data into training and 30% testing. Use training to fit RandomForestClassifier. And use testing subset to test the f1 score with macro averaging: first, predict the labels for test set using .predict(), then compare predicted to actual labels and calculate the score. Store this score in \"score\" variable, we will test it.\n",
    "\n",
    "Now let us compare the results to a robust estimate of performance using 5-fold cross validation (5 splits) with 100 repetitions (more repetitions will take time). Calculate 'f1_macro'  scores with cross validation for the whole dataset of X and y (not the training or testing subsets). Store the results of cross validation scores in \"scores\" variable (it will be tested). Let us now take the mean of \"scores\" and compare it to \"score\" that we calculated previously. What do you see?\n",
    "For visual aid plot distributions of scores across cross validation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cb35028af3d86e0921d48774c2038544",
     "grade": false,
     "grade_id": "cell-f01ff137236eed5f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "digits = sklearn.datasets.load_digits()\n",
    "\n",
    "# X - how digits are handwritten\n",
    "X = digits['data']\n",
    "\n",
    "# y - what these digits actually are\n",
    "y = digits['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "76b3aa740f34f592e69998775ea50d1f",
     "grade": false,
     "grade_id": "cell-9b650ceb67fbab9b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948520500932\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl/xuTw8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcdd1cf8128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clf = RandomForestClassifier(random_state = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, stratify = y)\n",
    "clf.fit(X_train, y_train)\n",
    "predicted_labels = clf.predict(X_test)\n",
    "score = f1_score(y_test, predicted_labels, average = 'macro')\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 100, random_state = 1)\n",
    "scores = cross_val_score(clf, X, y, cv = rskf, n_jobs = -1, scoring = \"f1_macro\") \n",
    "print(np.mean(scores))\n",
    "\n",
    "pd.Series(scores).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4c3121bb1463952c522585b84b9853a8",
     "grade": true,
     "grade_id": "cell-64444146993f782e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert X.shape == (1797, 64)\n",
    "assert y.shape == (1797, )\n",
    "assert score > 0.91\n",
    "assert np.percentile(scores, 5) < score < np.percentile(scores, 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ecaffc8e4c2ebf10b1488c06755b4c9b",
     "grade": false,
     "grade_id": "cell-39b7df583ce9ce92",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 6 (not graded) - Grid SearchCV\n",
    "\n",
    "Apply KNN classifier to digits dataset. You need to find the optimal parameters for KNN.\n",
    "Number of neighbors is not the only parameter that can be changed. As you can see from the first exercise the distance metric is an important parameter. Also, when choosing the class label by voting between K nearest neigbors different weights could be assigned to samples. A popular choice would be to assign higher weights to neighbors that are closer.\n",
    "\n",
    "Split the dataset into train_test and validation and only use the train_test portion for grid search CV.\n",
    "Use Grid Search cross validation to find the optimal combination of parameters listed below:\n",
    "\n",
    "n_neighbors:  3, 5, 7, 9, 11, 13, 15\n",
    "weights: uniform, distance\n",
    "metric: euclidean, manhattan, chebyshev\n",
    "\n",
    "In order to define what is an optimal solution you need to choose a score. The choice is up to you, but remember that digits dataset involves multiple classes. There is no assert test.\n",
    "\n",
    "You will need grid.best_params_ and grid.best_estimator_\n",
    "\n",
    "Use the best performing classifier grid.best_estimator_ to predict values for the validation portion of your dataset. Print a confusion matrix (use confusion_matrix function) and analyze which digits are easily confused with the other ones by the classifier.\n",
    "\n",
    "If you have questions about the solution mark them clearly and we will answer in okpy grading system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1c73eec3a43b99f9c3ed4ab1553b19f3",
     "grade": false,
     "grade_id": "cell-ccb9196a5de0a293",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "digits = sklearn.datasets.load_digits()\n",
    "\n",
    "# X - how digits are handwritten\n",
    "X = digits['data']\n",
    "\n",
    "# y - what these digits actually are\n",
    "y = digits['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "29650af10f742a016012324a3e8cffa5",
     "grade": true,
     "grade_id": "cell-22e9ce2d35f7301f",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters to use: KNeighborsClassifier(algorithm='auto', leaf_size=15, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='distance') \n",
      "\n",
      "Confusion matrix using f1 macro scoring \n",
      " [[54  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 55  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 53  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 55  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 53  0  0  1  0  0]\n",
      " [ 0  0  0  0  0 55  0  0  0  0]\n",
      " [ 1  0  0  0  0  0 53  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 53  0  1]\n",
      " [ 0  3  0  0  0  0  0  0 49  0]\n",
      " [ 0  0  0  2  1  0  0  0  1 50]] \n",
      "\n",
      "Parameters to use: KNeighborsClassifier(algorithm='auto', leaf_size=15, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='distance') \n",
      "\n",
      "Confusion matrix using f1 micro scoring \n",
      " [[54  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 55  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 53  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 55  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 53  0  0  1  0  0]\n",
      " [ 0  0  0  0  0 55  0  0  0  0]\n",
      " [ 1  0  0  0  0  0 53  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 53  0  1]\n",
      " [ 0  3  0  0  0  0  0  0 49  0]\n",
      " [ 0  0  0  2  1  0  0  0  1 50]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "knc = KNeighborsClassifier()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .3, stratify = y)\n",
    "#knc.fit(X_train, y_train)\n",
    "#knc.predict(X_test)\n",
    "\n",
    "parameters = {\"n_neighbors\": (3, 5, 7, 9, 11, 13, 15), # Does putting values in [] format make a difference?\n",
    "              \"weights\": (\"uniform\", \"distance\"),\n",
    "              \"leaf_size\": (15, 20, 25, 30, 35, 40),\n",
    "              \"metric\": (\"euclidean\", \"manhattan\", \"chebyshev\")}\n",
    "# see here for distance metrics:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n",
    "# and all relevant grid search info:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.predict\n",
    "\n",
    "grid_search_macro = GridSearchCV(knc, parameters, scoring = \"f1_macro\")\n",
    "grid_search_macro.fit(X_train, y_train)\n",
    "print(\"Parameters to use:\", grid_search_macro.best_estimator_, \"\\n\")\n",
    "y_pred = grid_search_macro.predict(X_test) # call on estimator with best parameters\n",
    "print(\"Confusion matrix using f1 macro scoring \\n\", confusion_matrix(y_test, y_pred), \"\\n\")\n",
    "\n",
    "# As stated in intro  by Alex:\n",
    "# In problems where infrequent classes are nonetheless important, macro-averaging\n",
    "# may be a means of highlighting their performance. On the other hand, \n",
    "# the assumption that all classes are equally important is often untrue, such \n",
    "# that macro-averaging will over-emphasize the typically low performance on an infrequent class.\n",
    "# Basically averages each class\n",
    "\n",
    "# For fun, what difference does f1_micro make? \n",
    "grid_search_micro = GridSearchCV(knc, parameters, scoring = \"f1_micro\")\n",
    "grid_search_micro.fit(X_train, y_train)\n",
    "print(\"Parameters to use:\", grid_search_micro.best_estimator_, \"\\n\")\n",
    "y_pred = grid_search_micro.predict(X_test)\n",
    "print(\"Confusion matrix using f1 micro scoring \\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
